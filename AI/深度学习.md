
## 标准卷积
输入 m 个尺寸为 h, w 的特征图，通过卷积计算获得 n 个通道尺寸为 与 h′ w′ 的特征图的计算过程。
![[Pasted image 20220920110534.png|900]]
![[Pasted image 20220920124814.png]]
一个卷积核中的 m 个卷积分 别与输入特征图的 m 个通道数据进行卷积计算，生成一个中间结果，然后 m 个中间结果 按位求和，最终就能获得 n 个输出特征图中的一个特征图。
![[Pasted image 20220920110605.png|900]]


## 深度可分离卷积
简单来说，深度可 分离卷积就是我们刚才所说的在效果近似相同的情况下，需要的计算量更少。

深度可分离卷积（Depthwise Separable Convolution）由 **Depthwise（DW）和 Pointwise（PW）这两部分卷积组合而成的。

#### Depthwise（DW）卷积
DW 卷积就是有 m 个卷积核的卷积，每个卷积核中的通道数为 1，这 m 个卷积核分别与输入特征图对应的通道数据做卷积运算，所以 DW 卷积的输出是 有 m 个通道的特征图。通常来说，DW 卷积核的大小是 3x3 的。
![[Pasted image 20220920110701.png|900]]
#### Pointwise（PW）卷积
通常来说，深度可分离卷积的目标是轻量化标准卷积计算的，所以它是可以来替换标准卷 积的，这也意味着原卷积计算的输出尺寸是什么样，替换后的输出尺寸也要保持一致。

所以，在深度可分离卷积中，我们最终要获得一个具有 n 个通道的输出特征图，而刚才介 绍的 DW 卷积显然没有达到，并且 DW 卷积也忽略了输入特征图通道之间的信息。

所以，在 DW 之后我们还要加一个 PW 卷积。PW 卷积也叫做逐点卷积。PW 卷积的主要 作用就是将 DW 输出的 m 个特征图结合在一起考虑，再输出一个具有 n 个通道的特征 图。

在卷积神经网络中，我们经常可以看到使用 1x1 的卷积，1x1 的卷积主要作用就是升维与 降维。所以，在 DW 的输出之后的 PW 卷积，就是 n 个卷积核的 1x1 的卷积，每个卷积 核中有 m 个通道的卷积数据。
![[Pasted image 20220920123852.png|900]]

#### 计算量
![[Pasted image 20220920124125.png|900]]
![[Pasted image 20220920124059.png|600]]
## 空洞卷积
空洞卷积经常用于图像分割任务当中。图像分割任务的目的是要做到 pixel-wise 的输出， 也就是说，对于图片中的每一个像素点，模型都要进行预测。

对于一个图像分割模型，通常会采用多层卷积来提取特征的，随着层数的不断加深，感受野也越来越大。

但是对于图像分割模型有个问题，经过多层的卷积与 pooling 操作之后，特征图会变小。 为了做到每个像素点都有预测输出，我们需要对较小的特征图进行上采样或反卷积，将特 征图扩大到一定尺度，然后再进行预测。

要知道，从一个较小的特征图恢复到一个较大的特征图，这显然会带来一定的信息损失， 特别是较小的物体，这种损失是很难恢复的。那问题来了，能不能既保证有比较大的感受野，同时又不用缩小特征图呢？

估计你已经猜到了，空洞卷积就是解决这个问题的杀手锏，它最大的优点就是不需要缩小特征图，也可以获得更大的感受野。
![[Pasted image 20220920124834.png]]
## 损失函数和代价函数
损失函数是评价拟合函数表现效果“好坏”的度量指标，是单个样本点的误差。

代价函数是在训练样本集合上，所有样本的拟合误差的平均值。

严格来说，损失函数的种类是无穷多的。这是因为损失函数是用来度量模型拟合效 果和真实值之间的差距，而度量方式要根据问题的特点或者需要优化的方面具体定制，所 以损失函数的种类是无穷无尽的。

常见的损失函数有：0-1 损失函数、平方损失函数、均方差损失函数和平均绝对误差损失函数、交叉熵损失函数、softmax 损失函数等。

## 前馈网络
前馈网络，也称为前馈神经网络。顾名思义，是一种“往前走”的神经网络。它是最简单 的神经网络，其典型特征是一个单向的多层结构。简化的结构如下图：
![[Pasted image 20220920141848.png|600]]
这个图中，你会看到最左侧的绿色 的一个个神经元，它们相当于第 0 层，一般适用于接收输入数据的层，所以我们把它们叫 做输入层。

比如我们要训练一个 y=f(x) 函数的神经网络，x 作为一个向量，就需要通过这个绿色的输 入层进入模型。那么在这个网络中，输入层有 5 个神经元，这意味着它可以接收一个 5 维 长度的向量。

网络的中间有一层红色的神经元，它们相当于模型的“内 部”，一般来说对外不可见，或者使用者并不关心的非结果部分，我们称之为隐藏层。在 实际的网络模型中，隐藏层会有非常多的层数，它们是网络最为关键的内部核心，也是模 型能够学习知识的关键部分。

在图的右侧，蓝色的神经元是网络的最后一层。模型内部计算完成之后，就需要通过这一 层输出到外部，所以也叫做输出层。

需要说明的是，神经元之间的连线，表示神经元之间连接的权重，通过权重就会知道网络 中每个节点的重要程度。

在前馈网络 中，数据从输入层进入到隐藏层的第一层，然后传播到第二层，第三层……一直到最后通过输出层输出。数据的传播是单向的，无法后退，只能前行。

## 反向传播
模型就是通过不断地减小损失函数值的方式来进行 学习的。让损失函数最小化，通常就要采用梯度下降的方式，即：每一次给模型的权重进行更新的时候，都要按照梯度的反方向进行。

为什么呢？因为梯度向量的方向即为函数值增长最快的方向，反方向则是减小最快的方向。

上面这个自然段的内容非常非常核心，为了确保你学会，我们换个方式再说一次：模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习。

反向传播算法（Backpropagation）是目前训练神经网络最常用且最有效的算法。模型就 是通过反向传播的方式来不断更新自身的参数，从而实现了“学习”知识的过程。

反向传播的主要原理是：

- 前向传播：数据从输入层经过隐藏层最后输出，其过程和之前讲过的前馈网络基本一 致。

- 计算误差并传播：计算模型输出结果和真实结果之间的误差，并将这种误差通过某种方 式反向传播，即从输出层向隐藏层传递并最后到达输入层。

- 迭代：在反向传播的过程中，根据误差不断地调整模型的参数值，并不断地迭代前面两 个步骤，直到达到模型结束训练的条件。

其中最重要的环节有两个：一是通过某种方式反向传播；二是根据误差不断地调整模型的 参数值。

这两个环节，我们统称为优化方法，一般而言，多采用梯度下降的方法。



## 三个深度学习神经网络性能优化的方法

第一个方法是针对图形数据做增广，也就是把有限的数据个数变多。其实不只是 CNN 网络，对于其他神经网络 模型，乃至神经网络模型之外的模型来说，数据的数量都是多多益善。数据的量越大，模 型出现过拟合的风险就越小。

第二个方法是在网络模型中增加一些 Dropout 层，这种类型的神经网络层，通过阻止各个 神经元之间的共同作用，阻止它们形成固定的特征提取模式来提高神经网络的泛化能力。

第三个方法是尝试使用不同类型的神经网络优化器，来克服网络训练时落入局部最低点的问题。其中，最常用的神经网络优化器是 Adam，但具体到每一个数据集来说，可能还有 更适合自己的优化器，因此，我建议你在实际项目中最好还是多试试不同的优化器。就目 前而言，RMSprop 和 Adam 都是常用的优化器，而 Adam 更是多种优化思路的集大成 者，一般情况下是优化器的首选项。
![[Pasted image 20220907221507.png|700]]
