
## CPU架构
![[Pasted image 20220909155038.png|700]]

## CPU的工作模式
按照 CPU 功能升级迭代的顺序，CPU 的工作模式有实模式、保护模式、长模式

1. 实模式，早期 CPU 是为了支持单道程序运行而实现的，单道程序能掌控计算机所有的资 源，早期的软件规模不大，内存资源也很少，所以实模式极其简单，仅支持 16 位地址空 间，分段的内存模型，对指令不加限制地运行，对内存没有保护隔离作用。 
2. 保护模式，随着多道程序的出现，就需要操作系统了。内存需求量不断增加，所以 CPU 实现了保护模式以支持这些需求。 保护模式包含特权级，对指令及其访问的资源进行控制，对内存段与段之间的访问进行严 格检查，没有权限的绝不放行，对中断的响应也要进行严格的权限检查，扩展了 CPU 寄存 器位宽，使之能够寻址 32 位的内存地址空间和处理 32 位的数据，从而 CPU 的性能大大 提高。 
3. 长模式，又名 AMD64 模式，最早由 AMD 公司制定。由于软件对 CPU 性能需求永无 止境，所以长模式在保护模式的基础上，把寄存器扩展到 64 位同时增加了一些寄存器，使 CPU 具有了能处理 64 位数据和寻址 64 位的内存地址空间的能力。 长模式弱化段模式管理，只保留了权限级别的检查，忽略了段基址和段长度，而地址的检 查则交给了 MMU。

## 金字塔存储层次
![[Pasted image 20220909155117.png|700]]

## CPU是如何选择线程执行的？
一个系统中可能会运行着非常多的线程，这些线程数可能远超系统中的 CPU 核数，这时候这些任务就需要排队，每个 CPU 都会维护着自己运行队列（runqueue）里的 线程。这个运行队列的结构大致如下图所示：
![[Pasted image 20220909160114.png]]
每个 CPU 都有自己的运行队列（runqueue），需要运行的线程会被加入到这个队列中。 因为有些线程的优先级高，Linux 内核为了保障这些高优先级任务的执行，设置了不同的调 度类（Scheduling Class），如下所示：
![[Pasted image 20220909160139.png]]
这几个调度类的优先级如下：Deadline > Realtime > Fair。Linux 内核在选择下一个任务 执行时，会按照该顺序来进行选择，也就是先从 dl_rq 里选择任务，然后从 rt_rq 里选择任 务，最后从 cfs_rq 里选择任务。所以实时任务总是会比普通任务先得到执行。

如果你不做任何设置的话，用户线程在默认情况下都是普通线程，也就是属于 Fair 调度 类，由 CFS 调度器来进行管理。CFS 调度器的目的是为了实现线程运行的公平性，举个例 子，假设一个 CPU 上有两个线程需要执行，那么每个线程都将分配 50% 的 CPU 时间， 以保障公平性。其实，各个线程之间执行时间的比例，也是可以人为干预的，比如在 Linux 上可以调整进程的 nice 值来干预，从而让优先级高一些的线程执行更多时间。这就是 CFS 调度器的大致思想。

## CPU使用率
我们通常所说的 CPU 使用率，就是除了空闲时间外的其他时间占总 CPU 时间的百分比。

![[Pasted image 20220826212543.png|800]]

假设一个用户程序开始运行了，那么就对应着第一个"us"框，"us"是"user"的缩写，代表Linux 的用户态 CPU Usage。普通用户程序代码中，只要不是调用系统调用（System Call），这些代码的指令消耗的 CPU 就都属于"us"。

当这个用户程序代码中调用了系统调用，比如说 read() 去读取一个文件，这时候这个用户进程就会从用户态切换到内核态。

内核态 read() 系统调用在读到真正 disk 上的文件前，就会进行一些文件系统层的操作。那么这些代码指令的消耗就属于"sy"，这里就对应上面图里的第二个框。"sy"是"system"的缩写，代表内核态 CPU 使用。

接下来，这个 read() 系统调用会向 Linux 的 Block Layer 发出一个 I/O Request，触发一个真正的磁盘读取操作。

这时候，这个进程一般会被置为 TASK_UNINTERRUPTIBLE。而 Linux 会把这段时间标示 成"wa"，对应图中的第三个框。"wa"是"iowait"的缩写，代表等待 I/O 的时间，这里的I/O 是指 Disk I/O。

紧接着，当磁盘返回数据时，进程在内核态拿到数据，这里仍旧是内核态的 CPU 使用中 的"sy"，也就是图中的第四个框。

然后，进程再从内核态切换回用户态，在用户态得到文件数据，这里进程又回到用户态的 CPU 使用，"us"，对应图中第五个框。

好，这里我们假设一下，这个用户进程在读取数据之后，没事可做就休眠了。并且我们可以进一步假设，这时在这个 CPU 上也没有其他需要运行的进程了，那么系统就会进入"id"这个步骤，也就是第六个框。"id"是"idle"的缩写，代表系统处于空闲状态。

如果这时这台机器在网络收到一个网络数据包，网卡就会发出一个中断（interrupt）。相应地，CPU 会响应中断，然后进入中断服务程序。

这时，CPU 就会进入"hi"，也就是第七个框。"hi"是"hardware irq"的缩写，代表 CPU 处理硬中断的开销。由于我们的中断服务处理需要关闭中断，所以这个硬中断的时间不能太长。

但是，发生中断后的工作是必须要完成的，如果这些工作比较耗时那怎么办呢？Linux 中有 一个软中断的概念（softirq），它可以完成这些耗时比较长的工作。

你可以这样理解这个软中断，从网卡收到数据包的大部分工作，都是通过软中断来处理的。那么，CPU 就会进入到第八个框，"si"。这里"si"是"softirq"的缩写，代表 CPU 处理软中断的开销

还有两个类型：
一个是"ni"，是"nice"的缩写，这里表示如果进程的 nice 值是正值（1-19），代表优先级比较低的进程运行时所占用的 CPU。

另外一个是"st"，"st"是"steal"的缩写，是在虚拟机里用的一个 CPU 使用类型，表示有多少时间是被同一个宿主机上的其他虚拟机抢走的。
![[Pasted image 20220826212942.png|800]]

## top命令
![[Pasted image 20220909161320.png]]
对于应用而言，它的目标是让 CPU 的开销尽量用在执行用户代码上，而非其他方面。usr 利用率越高，说明 CPU 的效率 越高。如果 usr 低，就说明 CPU 执行应用的效率不高。

#### 平均负载load average   VS   CPU使用率CPU Usage
简单来说，平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数， 也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。

所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。

不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比 如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态 （Uninterruptible Sleep，也称为 Disk Sleep）的进程。

比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不 能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。如果此时的进程被打 断了，就容易出现磁盘数据与进程数据不一致的问题。

所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制。

平均负载最理想的情况是等于 CPU 个数。在我看来，当**平均负载高于 CPU 数量 70%** 的时候，你就应该分析排查负载高的问题了。 一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。

**平均负载高有可能是 CPU 密集型进程导致的；平均负载高并不一定代表 CPU 使用率高，还有可能是 I/O 更繁忙了；**

#### 平均负载与 CPU 使用率
平均负载是指单位时间内，处于可运行状态和不可中 断状态的进程数。所以，它不仅包括了**正在使用 CPU 的进程**，还包括**等待 CPU** 和**等待 I/O 的进程**。

而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如：
- CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；
- I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高；
- 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。

第一，不论计算机 CPU 是空闲还是满负载，Load Average 都是 Linux 进程调度器中可运行队列（Running Queue）里的一段时间的平均进程数目。

第二，计算机上的 CPU 还有空闲的情况下，CPU Usage 可以直接反映到"load average"上，什么是 CPU 还有空闲呢？具体来说就是可运行队列中的进程数目小于 CPU 个数，这种情况下，单位时间进程 CPU Usage 相加的平均值应该就是"load average"的 值。

第三，计算机上的 CPU 满负载的情况下，计算机上的 CPU 已经是满负载了，同时还有更多的进程在排队需要 CPU 资源。这时"load average"就不能和 CPU Usage 等同了。

平均负载load average统计了这两种情况的进程：
第一种是 Linux 进程调度器中可运行队列（Running Queue）一段时间（1 分钟，5 分 钟，15 分钟的进程平均数。
第二种是 Linux 进程调度器中休眠队列（Sleeping Queue）里的一段时间的 TASK_UNINTERRUPTIBLE 状态下的进程平均数。

所以，最后的公式就是：Load Average= 可运行队列进程平均数 + 休眠队列中不可打断的进程平均数
![[Pasted image 20220826221705.png|700]]
在其他 Unix 操作系统里 Load Average 只考虑 CPU 部分，Load Average 计算的是进程调度器中可运行队列（Running Queue）里的一段时间（1 分钟，5 分钟，15 分钟）的平均进程数目，而 Linux 在这个基础上，又加上了进程调度器中休眠队列（Sleeping Queue）里的一段时间的 TASK_UNINTERRUPTIBLE 状态的平均进程数目。

## 软中断和硬中断的区别
软中断是用来处理硬中断在短时间内无法完成的任务的。硬中断由于执行时间短，所以如果它的发生频率不高的话，一般不会给业务带来明显影响。但是由于内核里关中断的地方太多，所以进程往往会给硬中断带来 一些影响，比如进程关中断时间太长会导致网络报文无法及时处理，进而引起业务性能抖动。

我们在生产环境中就遇到过这种关中断时间太长引起的抖动案例，比如 cat /proc/slabinfo 这个操作里的逻辑关中断太长，它会致使业务 RT 抖动。这是因为该命令会统计系统中所有 的 slab 数量，并显示出来，在统计的过程中会关中断。如果系统中的 slab 数量太多，就会导致关中断的时间太长，进而引起网络包阻塞，ping 延迟也会因此明显变大。所以，在 生产环境中我们要尽量避免去采集 /proc/slabinfo，否则可能会引起业务抖动。

相比硬中断，软中断的执行时间会长一些，而且它也会抢占正在执行进程的 CPU，从而导致进程在它运行期间只能等待。所以，相对而言它会更容易给业务带来延 迟。

为了避免软中断太过频繁，进程无法得到 CPU 而被饿死的情况，内核引入了 ksoftirqd 这 个机制。如果所有的软中断在短时间内无法被处理完，内核就会唤醒 ksoftirqd 处理接下来的软中断。ksoftirqd 与普通进程的优先级一样，nice值为0，也就是说它会和普通进程公平地使用 CPU，这在一定程度上可以避免用户进程被饿死的情况，特别是对于那些更高优先级的实时用户进程而言。

RPS （Receivce Packet Steering）模拟网卡多队列的本质是网卡特性 unload 到 CPU，靠牺牲 CPU 时间来提升吞吐。如果你的系统已经很繁忙了，那么再使用该特性无疑是雪 上加霜。所以，需要注意，使用 RPS 的前提条件是：系统的整体 CPU 利用率不能太 高。如果你的网卡支持了硬件多队列，那么就可以直接使用硬件多队列了。

## 保护环（Protection Ring）系统调用
x86 CPU 实现了 4 个级别的保护环，也就是 ring 0 到 ring 3，其中 ring 0 权限最 大，ring 3 最小。就拿 Linux 来说，它的内核态就运行在 ring 0，只有内核态可以操作 CPU 的所有指令。Linux 的用户态运行在 ring 3，它就没办法操作很多核心指令。

内核空间（Ring 0）具有最高权限，可以直接访问所有资源；

用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统 调用陷入到内核中，才能访问这些特权资源。
![[Pasted image 20220914210622.png|600]]
那么处于 ring 3 的应用程序也想要运行 ring 0 的指令的话，该怎么办到呢？就是让内核去间接地帮忙。而这个“忙”，其实就是通过系统调用来“帮”的。

这样的话，用户空间程序就可以借系统调用之手，完成它原本没有权限完成的指令（进入内核态）。

既然，系统调用要给用户空间的应用程序提供丰富而全面的接口，无论是文件和网络等 IO 操作，还是像申请内存等更加核心的操作，都需要通过系统调用来完成，那么系统调用的数量肯定是不少的。比如 Linux 的系统调用数量大约在 300 多个，有的操作系统则会达到 500 个以上。

系统调用的过程是：CPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码， CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。
而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，**一次系统调用的过程，其实是发生了两次 CPU 上下文切换**

所以，系统调用过程通常称为特权模式切换，而不是上下文切换。但实际上，系统调用过程 中，CPU 的上下文切换还是无法避免的。


## CPU 上下文切换
Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。当 然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流 分配给它们，造成多任务同时运行的错觉。

而在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说， 需要系统事先帮它设置好 CPU 寄存器和程序计数器（Program Counter，PC）。

CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 CPU 上下文。
![[Pasted image 20220915212046.png]]
CPU 上下文切换，就 是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加 载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。

而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。 这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。

根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是进程上下文切换、线程上下文切换以及中断上下文切换。

CPU 上下文切换，是保证 Linux 系统正常工作的核心功能之一，一般情况下不需要我们特别关注。但过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，从而缩短进程真正运行的时间，导致系统的整体性能大幅下降。