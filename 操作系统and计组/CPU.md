
## CPU架构
![[Pasted image 20220909155038.png|700]]

## 金字塔存储层次
![[Pasted image 20220909155117.png|700]]

## CPU是如何选择线程执行的？
一个系统中可能会运行着非常多的线程，这些线程数可能远超系统中的 CPU 核 数，这时候这些任务就需要排队，每个 CPU 都会维护着自己运行队列（runqueue）里的 线程。这个运行队列的结构大致如下图所示：
![[Pasted image 20220909160114.png]]
每个 CPU 都有自己的运行队列（runqueue），需要运行的线程会被加入到这个队列中。 因为有些线程的优先级高，Linux 内核为了保障这些高优先级任务的执行，设置了不同的调 度类（Scheduling Class），如下所示：
![[Pasted image 20220909160139.png]]
这几个调度类的优先级如下：Deadline > Realtime > Fair。Linux 内核在选择下一个任务 执行时，会按照该顺序来进行选择，也就是先从 dl_rq 里选择任务，然后从 rt_rq 里选择任 务，最后从 cfs_rq 里选择任务。所以实时任务总是会比普通任务先得到执行。

如果你不做任何设置的话，用户线程在默认情况下都是普通线程，也就是属于 Fair 调度 类，由 CFS 调度器来进行管理。CFS 调度器的目的是为了实现线程运行的公平性，举个例 子，假设一个 CPU 上有两个线程需要执行，那么每个线程都将分配 50% 的 CPU 时间， 以保障公平性。其实，各个线程之间执行时间的比例，也是可以人为干预的，比如在 Linux 上可以调整进程的 nice 值来干预，从而让优先级高一些的线程执行更多时间。这就是 CFS 调度器的大致思想。

## CPU使用率
![[Pasted image 20220826212543.png|800]]

假设一个用户程序开始运行了，那么就对应着第一个"us"框，"us"是"user"的缩写，代表Linux 的用户态 CPU Usage。普通用户程序代码中，只要不是调用系统调用（System Call），这些代码的指令消耗的 CPU 就都属于"us"。

当这个用户程序代码中调用了系统调用，比如说 read() 去读取一个文件，这时候这个用户进程就会从用户态切换到内核态。

内核态 read() 系统调用在读到真正 disk 上的文件前，就会进行一些文件系统层的操作。那么这些代码指令的消耗就属于"sy"，这里就对应上面图里的第二个框。"sy"是"system"的缩写，代表内核态 CPU 使用。

接下来，这个 read() 系统调用会向 Linux 的 Block Layer 发出一个 I/O Request，触发一个真正的磁盘读取操作。

这时候，这个进程一般会被置为 TASK_UNINTERRUPTIBLE。而 Linux 会把这段时间标示 成"wa"，对应图中的第三个框。"wa"是"iowait"的缩写，代表等待 I/O 的时间，这里的I/O 是指 Disk I/O。

紧接着，当磁盘返回数据时，进程在内核态拿到数据，这里仍旧是内核态的 CPU 使用中 的"sy"，也就是图中的第四个框。

然后，进程再从内核态切换回用户态，在用户态得到文件数据，这里进程又回到用户态的 CPU 使用，"us"，对应图中第五个框。

好，这里我们假设一下，这个用户进程在读取数据之后，没事可做就休眠了。并且我们可 以进一步假设，这时在这个 CPU 上也没有其他需要运行的进程了，那么系统就会进入"id"这个步骤，也就是第六个框。"id"是"idle"的缩写，代表系统处于空闲状态。

如果这时这台机器在网络收到一个网络数据包，网卡就会发出一个中断（interrupt）。相 应地，CPU 会响应中断，然后进入中断服务程序。

这时，CPU 就会进入"hi"，也就是第七个框。"hi"是"hardware irq"的缩写，代表 CPU 处 理硬中断的开销。由于我们的中断服务处理需要关闭中断，所以这个硬中断的时间不能太长。

但是，发生中断后的工作是必须要完成的，如果这些工作比较耗时那怎么办呢？Linux 中有 一个软中断的概念（softirq），它可以完成这些耗时比较长的工作。

你可以这样理解这个软中断，从网卡收到数据包的大部分工作，都是通过软中断来处理 的。那么，CPU 就会进入到第八个框，"si"。这里"si"是"softirq"的缩写，代表 CPU 处理 软中断的开销

一个是"ni"，是"nice"的缩写，这里表示如果进程的 nice 值是正值（1-19），代表优先级 比较低的进程运行时所占用的 CPU。

另外一个是"st"，"st"是"steal"的缩写，是在虚拟机里用的一个 CPU 使用类型，表示有多 少时间是被同一个宿主机上的其他虚拟机抢走的。
![[Pasted image 20220826212942.png|800]]

## top命令
![[Pasted image 20220909161320.png]]
对于应用而言，它的目标是让 CPU 的开销尽量用在执行用户代码上，而非其他方面。usr 利用率越高，说明 CPU 的效率 越高。如果 usr 低，就说明 CPU 执行应用的效率不高。

#### load average   VS   CPU Usage
第一，不论计算机 CPU 是空闲还是满负载，Load Average 都是 Linux 进程调度器中可运 行队列（Running Queue）里的一段时间的平均进程数目。

第二，计算机上的 CPU 还有空闲的情况下，CPU Usage 可以直接反映到"load average"上，什么是 CPU 还有空闲呢？具体来说就是可运行队列中的进程数目小于 CPU 个数，这种情况下，单位时间进程 CPU Usage 相加的平均值应该就是"load average"的 值。

第三，计算机上的 CPU 满负载的情况下，计算机上的 CPU 已经是满负载了，同时还有更 多的进程在排队需要 CPU 资源。这时"load average"就不能和 CPU Usage 等同了。

平均负载load average统计了这两种情况的进程：
第一种是 Linux 进程调度器中可运行队列（Running Queue）一段时间（1 分钟，5 分 钟，15 分钟）的进程平均数。
第二种是 Linux 进程调度器中休眠队列（Sleeping Queue）里的一段时间的 TASK_UNINTERRUPTIBLE 状态下的进程平均数。

所以，最后的公式就是：Load Average= 可运行队列进程平均数 + 休眠队列中不可打断的进程平均数
![[Pasted image 20220826221705.png|700]]
在其他 Unix 操作系统里 Load Average 只考虑 CPU 部分，Load Average 计算的是进程 调度器中可运行队列（Running Queue）里的一段时间（1 分钟，5 分钟，15 分钟）的平 均进程数目，而 Linux 在这个基础上，又加上了进程调度器中休眠队列（Sleeping Queue）里的一段时间的 TASK_UNINTERRUPTIBLE 状态的平均进程数目。

## 软中断和硬中断的区别
软中断是用来处理硬中断在短时间内无法完成的任务的。硬中断由于执行时间短，所以如果它的发生频率不高的话，一般不会给业务带来明显影响。但是由于内核里关中断的地方太多，所以进程往往会给硬中断带来 一些影响，比如进程关中断时间太长会导致网络报文无法及时处理，进而引起业务性能抖动。

我们在生产环境中就遇到过这种关中断时间太长引起的抖动案例，比如 cat /proc/slabinfo 这个操作里的逻辑关中断太长，它会致使业务 RT 抖动。这是因为该命令会统计系统中所有 的 slab 数量，并显示出来，在统计的过程中会关中断。如果系统中的 slab 数量太多，就会导致关中断的时间太长，进而引起网络包阻塞，ping 延迟也会因此明显变大。所以，在 生产环境中我们要尽量避免去采集 /proc/slabinfo，否则可能会引起业务抖动。

相比硬中断，软中断的执行时间会长一些，而且它也会抢占正在执行进程的 CPU，从而导致进程在它运行期间只能等待。所以，相对而言它会更容易给业务带来延 迟。

为了避免软中断太过频繁，进程无法得到 CPU 而被饿死的情况，内核引入了 ksoftirqd 这 个机制。如果所有的软中断在短时间内无法被处理完，内核就会唤醒 ksoftirqd 处理接下来的软中断。ksoftirqd 与普通进程的优先级一样，nice值为0，也就是说它会和普通进程公平地使用 CPU，这在一定程度上可以避免用户进程被饿死的情况，特别是对于那些更高优先级的实时用户进程而言。

RPS （Receivce Packet Steering）模拟网卡多队列的本质是网卡特性 unload 到 CPU，靠牺牲 CPU 时间来提升吞吐。如果你的系统已经很繁忙了，那么再使用该特性无疑是雪 上加霜。所以，需要注意，使用 RPS 的前提条件是：系统的整体 CPU 利用率不能太 高。如果你的网卡支持了硬件多队列，那么就可以直接使用硬件多队列了。

## 保护环（Protection Ring）
x86 CPU 实现了 4 个级别的保护环，也就是 ring 0 到 ring 3，其中 ring 0 权限最 大，ring 3 最小。就拿 Linux 来说，它的内核态就运行在 ring 0，只有内核态可以操作 CPU 的所有指令。Linux 的用户态运行在 ring 3，它就没办法操作很多核心指令。
![[Pasted image 20220914210622.png|600]]
那么处于 ring 3 的应用程序也想要运行 ring 0 的指令的话，该怎么办到呢？就是让内核去 间接地帮忙。而这个“忙”，其实就是通过系统调用来“帮”的。

这样的话，用户空间程序就可以借系统调用之手，完成它原本没有权限完成的指令（进入 内核态）。

既然，系统调用要给用户空间的应用程序提供丰富而全面的接口，无论是文件和网络等 IO 操作，还是像申请内存等更加核心的操作，都需要通过系统调用来完成，那么系统调用的 数量肯定是不少的。比如 Linux 的系统调用数量大约在 300 多个，有的操作系统则会达到 500 个以上。