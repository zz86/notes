
## CPU架构
![[Pasted image 20220909155038.png|700]]

## CPU的工作模式
按照 CPU 功能升级迭代的顺序，CPU 的工作模式有实模式、保护模式、长模式

1. 实模式，早期 CPU 是为了支持单道程序运行而实现的，单道程序能掌控计算机所有的资 源，早期的软件规模不大，内存资源也很少，所以实模式极其简单，仅支持 16 位地址空 间，分段的内存模型，对指令不加限制地运行，对内存没有保护隔离作用。 
2. 保护模式，随着多道程序的出现，就需要操作系统了。内存需求量不断增加，所以 CPU 实现了保护模式以支持这些需求。 保护模式包含特权级，对指令及其访问的资源进行控制，对内存段与段之间的访问进行严 格检查，没有权限的绝不放行，对中断的响应也要进行严格的权限检查，扩展了 CPU 寄存 器位宽，使之能够寻址 32 位的内存地址空间和处理 32 位的数据，从而 CPU 的性能大大 提高。 
3. 长模式，又名 AMD64 模式，最早由 AMD 公司制定。由于软件对 CPU 性能需求永无 止境，所以长模式在保护模式的基础上，把寄存器扩展到 64 位同时增加了一些寄存器，使 CPU 具有了能处理 64 位数据和寻址 64 位的内存地址空间的能力。 长模式弱化段模式管理，只保留了权限级别的检查，忽略了段基址和段长度，而地址的检 查则交给了 MMU。

## 金字塔存储层次
![[Pasted image 20220909155117.png|700]]
![[Pasted image 20220917110136.png]]

## CPU是如何选择线程执行的？
一个系统中可能会运行着非常多的线程，这些线程数可能远超系统中的 CPU 核数，这时候这些任务就需要排队，每个 CPU 都会维护着自己运行队列（runqueue）里的 线程。这个运行队列的结构大致如下图所示：
![[Pasted image 20220909160114.png]]
每个 CPU 都有自己的运行队列（runqueue），需要运行的线程会被加入到这个队列中。 因为有些线程的优先级高，Linux 内核为了保障这些高优先级任务的执行，设置了不同的调 度类（Scheduling Class），如下所示：
![[Pasted image 20220909160139.png]]
这几个调度类的优先级如下：Deadline > Realtime > Fair。Linux 内核在选择下一个任务 执行时，会按照该顺序来进行选择，也就是先从 dl_rq 里选择任务，然后从 rt_rq 里选择任 务，最后从 cfs_rq 里选择任务。所以实时任务总是会比普通任务先得到执行。

如果你不做任何设置的话，用户线程在默认情况下都是普通线程，也就是属于 Fair 调度 类，由 CFS 调度器来进行管理。CFS 调度器的目的是为了实现线程运行的公平性，举个例 子，假设一个 CPU 上有两个线程需要执行，那么每个线程都将分配 50% 的 CPU 时间， 以保障公平性。其实，各个线程之间执行时间的比例，也是可以人为干预的，比如在 Linux 上可以调整进程的 nice 值来干预，从而让优先级高一些的线程执行更多时间。这就是 CFS 调度器的大致思想。

## CPU使用率
我们通常所说的 CPU 使用率，就是除了空闲时间外的其他时间占总 CPU 时间的百分比。

- 用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率 （nice），表示 CPU 在用户态运行的时间百分比。用户 CPU 使用率高，通常说明有应 用程序比较繁忙。
- 系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断）。系统 CPU 使 用率高，说明内核比较繁忙。
- 等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长。
- 软中断和硬中断的 CPU 使用率，分别表示内核调用软中断处理程序、硬中断处理程序的 时间百分比。它们的使用率高，通常说明系统发生了大量的中断。
- 除了上面这些，还有在虚拟化环境中会用到的窃取 CPU 使用率（steal）和客户 CPU 使 用率（guest），分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比。

![[Pasted image 20220826212543.png|800]]

假设一个用户程序开始运行了，那么就对应着第一个"us"框，"us"是"user"的缩写，代表Linux 的用户态 CPU Usage。普通用户程序代码中，只要不是调用系统调用（System Call），这些代码的指令消耗的 CPU 就都属于"us"。

当这个用户程序代码中调用了系统调用，比如说 read() 去读取一个文件，这时候这个用户进程就会从用户态切换到内核态。

内核态 read() 系统调用在读到真正 disk 上的文件前，就会进行一些文件系统层的操作。那么这些代码指令的消耗就属于"sy"，这里就对应上面图里的第二个框。"sy"是"system"的缩写，代表内核态 CPU 使用。

接下来，这个 read() 系统调用会向 Linux 的 Block Layer 发出一个 I/O Request，触发一个真正的磁盘读取操作。

这时候，这个进程一般会被置为 TASK_UNINTERRUPTIBLE。而 Linux 会把这段时间标示 成"wa"，对应图中的第三个框。"wa"是"iowait"的缩写，代表等待 I/O 的时间，这里的I/O 是指 Disk I/O。

紧接着，当磁盘返回数据时，进程在内核态拿到数据，这里仍旧是内核态的 CPU 使用中 的"sy"，也就是图中的第四个框。

然后，进程再从内核态切换回用户态，在用户态得到文件数据，这里进程又回到用户态的 CPU 使用，"us"，对应图中第五个框。

好，这里我们假设一下，这个用户进程在读取数据之后，没事可做就休眠了。并且我们可以进一步假设，这时在这个 CPU 上也没有其他需要运行的进程了，那么系统就会进入"id"这个步骤，也就是第六个框。"id"是"idle"的缩写，代表系统处于空闲状态。

如果这时这台机器在网络收到一个网络数据包，网卡就会发出一个中断（interrupt）。相应地，CPU 会响应中断，然后进入中断服务程序。

这时，CPU 就会进入"hi"，也就是第七个框。"hi"是"hardware irq"的缩写，代表 CPU 处理硬中断的开销。由于我们的中断服务处理需要关闭中断，所以这个硬中断的时间不能太长。

但是，发生中断后的工作是必须要完成的，如果这些工作比较耗时那怎么办呢？Linux 中有 一个软中断的概念（softirq），它可以完成这些耗时比较长的工作。

你可以这样理解这个软中断，从网卡收到数据包的大部分工作，都是通过软中断来处理的。那么，CPU 就会进入到第八个框，"si"。这里"si"是"softirq"的缩写，代表 CPU 处理软中断的开销

还有两个类型：
一个是"ni"，是"nice"的缩写，这里表示如果进程的 nice 值是正值（1-19），代表优先级比较低的进程运行时所占用的 CPU。

另外一个是"st"，"st"是"steal"的缩写，是在虚拟机里用的一个 CPU 使用类型，表示有多少时间是被同一个宿主机上的其他虚拟机抢走的。
![[Pasted image 20220826212942.png|800]]

## top命令
![[Pasted image 20220909161320.png]]
对于应用而言，它的目标是让 CPU 的开销尽量用在执行用户代码上，而非其他方面。usr 利用率越高，说明 CPU 的效率越高。如果 usr 低，就说明 CPU 执行应用的效率不高。

#### 平均负载load average   VS   CPU使用率 CPU Usage
简单来说，平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数， 也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。

所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。

不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比 如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态 （Uninterruptible Sleep，也称为 Disk Sleep）的进程。

比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不 能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。如果此时的进程被打 断了，就容易出现磁盘数据与进程数据不一致的问题。

所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制。

平均负载最理想的情况是等于 CPU 个数。在我看来，当**平均负载高于 CPU 数量 70%** 的时候，你就应该分析排查负载高的问题了。 一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。

**平均负载高有可能是 CPU 密集型进程导致的；平均负载高并不一定代表 CPU 使用率高，还有可能是 I/O 更繁忙了；**

#### 平均负载与 CPU 使用率
平均负载是指单位时间内，处于可运行状态和不可中 断状态的进程数。所以，它不仅包括了**正在使用 CPU 的进程**，还包括**等待 CPU** 和**等待 I/O 的进程**。

而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如：
- CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；
- I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高；
- 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。

第一，不论计算机 CPU 是空闲还是满负载，Load Average 都是 Linux 进程调度器中可运行队列（Running Queue）里的一段时间的平均进程数目。

第二，计算机上的 CPU 还有空闲的情况下，CPU Usage 可以直接反映到"load average"上，什么是 CPU 还有空闲呢？具体来说就是可运行队列中的进程数目小于 CPU 个数，这种情况下，单位时间进程 CPU Usage 相加的平均值应该就是"load average"的 值。

第三，计算机上的 CPU 满负载的情况下，计算机上的 CPU 已经是满负载了，同时还有更多的进程在排队需要 CPU 资源。这时"load average"就不能和 CPU Usage 等同了。

平均负载load average统计了这两种情况的进程：
第一种是 Linux 进程调度器中可运行队列（Running Queue）一段时间（1 分钟，5 分 钟，15 分钟的进程平均数。
第二种是 Linux 进程调度器中休眠队列（Sleeping Queue）里的一段时间的 TASK_UNINTERRUPTIBLE 状态下的进程平均数。

所以，最后的公式就是：Load Average= 可运行队列进程平均数 + 休眠队列中不可打断的进程平均数
![[Pasted image 20220826221705.png|700]]
在其他 Unix 操作系统里 Load Average 只考虑 CPU 部分，Load Average 计算的是进程调度器中可运行队列（Running Queue）里的一段时间（1 分钟，5 分钟，15 分钟）的平均进程数目，而 Linux 在这个基础上，又加上了进程调度器中休眠队列（Sleeping Queue）里的一段时间的 TASK_UNINTERRUPTIBLE 状态的平均进程数目。

## 软中断和硬中断
中断是一种异步的事件处理机制，用来提高系统的并发处理能力。由于中断处理程序会打断其他进程的运行，所以，为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行。中断事件发生，会触发执行中断处理程序，而中断处理程序被分为上半部和下半部这两个部分。

上半部对应硬中断，用来快速处理中断，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间 敏感的工作。；

下半部对应软中断，用来异步处理上半部未完成的工作，通常以内核线程的方式运行。

Linux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，我们可以查看 proc 文 件系统中的 /proc/softirqs ，观察软中断的运行情况。

在 Linux 中，每个 CPU 都对应一个软中断内核线程，名字是 ksoftirqd/CPU 编号。当软中断事件的频率过高时，内核线程也会因为 CPU 使用率过高而导致软中断处理不及时，进而引发网络收发延迟、调度缓慢等性能问题。

例如网卡收到数据包：
- 网卡接收到数据包后，会通过硬件中断的方式，通知内核有新的数据到了。这时，内核就应 该调用中断处理程序来响应它。你可以自己先想一下，这种情况下的上半部和下半部分别负 责什么工作呢？
- 对上半部来说，既然是快速处理，其实就是要把网卡的数据读到内存中，然后更新一下硬件 寄存器的状态（表示数据已经读好了），最后再发送一个软中断信号，通知下半部做进一步 的处理。
- 而下半部被软中断信号唤醒后，需要从内存中找到网络数据，再按照网络协议栈，对数据进 行逐层解析和处理，直到把它送给应用程序。
- 所以，这两个阶段你也可以这样理解：
	- 上半部直接处理硬件请求，也就是我们常说的硬中断，特点是快速执行；
	- 而下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行。

实际上，上半部会打断 CPU 正在执行的任务，然后立即执行中断处理程序。而下半部以内 核线程的方式执行，并且每个 CPU 都对应一个软中断内核线程，名字为 “ksoftirqd/CPU 编号”，比如说， 0 号 CPU 对应的软中断内核线程的名字就是 ksoftirqd/0。

软中断 CPU 使用率过高也是一种最常见的性能问题。

软中断是用来处理硬中断在短时间内无法完成的任务的。硬中断由于执行时间短，所以如果它的发生频率不高的话，一般不会给业务带来明显影响。但是由于内核里关中断的地方太多，所以进程往往会给硬中断带来 一些影响，比如进程关中断时间太长会导致网络报文无法及时处理，进而引起业务性能抖动。

我们在生产环境中就遇到过这种关中断时间太长引起的抖动案例，比如 cat /proc/slabinfo 这个操作里的逻辑关中断太长，它会致使业务 RT 抖动。这是因为该命令会统计系统中所有 的 slab 数量，并显示出来，在统计的过程中会关中断。如果系统中的 slab 数量太多，就会导致关中断的时间太长，进而引起网络包阻塞，ping 延迟也会因此明显变大。所以，在 生产环境中我们要尽量避免去采集 /proc/slabinfo，否则可能会引起业务抖动。

相比硬中断，软中断的执行时间会长一些，而且它也会抢占正在执行进程的 CPU，从而导致进程在它运行期间只能等待。所以，相对而言它会更容易给业务带来延 迟。

为了避免软中断太过频繁，进程无法得到 CPU 而被饿死的情况，内核引入了 ksoftirqd 这 个机制。如果所有的软中断在短时间内无法被处理完，内核就会唤醒 ksoftirqd 处理接下来的软中断。ksoftirqd 与普通进程的优先级一样，nice值为0，也就是说它会和普通进程公平地使用 CPU，这在一定程度上可以避免用户进程被饿死的情况，特别是对于那些更高优先级的实时用户进程而言。

## 保护环（Protection Ring）系统调用
x86 CPU 实现了 4 个级别的保护环，也就是 ring 0 到 ring 3，其中 ring 0 权限最 大，ring 3 最小。就拿 Linux 来说，它的内核态就运行在 ring 0，只有内核态可以操作 CPU 的所有指令。Linux 的用户态运行在 ring 3，它就没办法操作很多核心指令。

内核空间（Ring 0）具有最高权限，可以直接访问所有资源；

用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统 调用陷入到内核中，才能访问这些特权资源。
![[Pasted image 20220914210622.png|600]]
那么处于 ring 3 的应用程序也想要运行 ring 0 的指令的话，该怎么办到呢？就是让内核去间接地帮忙。而这个“忙”，其实就是通过系统调用来“帮”的。

这样的话，用户空间程序就可以借系统调用之手，完成它原本没有权限完成的指令（进入内核态）。

既然，系统调用要给用户空间的应用程序提供丰富而全面的接口，无论是文件和网络等 IO 操作，还是像申请内存等更加核心的操作，都需要通过系统调用来完成，那么系统调用的数量肯定是不少的。比如 Linux 的系统调用数量大约在 300 多个，有的操作系统则会达到 500 个以上。

系统调用的过程是：CPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码， CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。
而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，**一次系统调用的过程，其实是发生了两次 CPU 上下文切换**

所以，系统调用过程通常称为特权模式切换，而不是上下文切换。但实际上，系统调用过程 中，CPU 的上下文切换还是无法避免的。


## CPU 上下文切换
Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。当 然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流 分配给它们，造成多任务同时运行的错觉。

而在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说， 需要系统事先帮它设置好 CPU 寄存器和程序计数器（Program Counter，PC）。

CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 CPU 上下文。
![[Pasted image 20220915212046.png]]
CPU 上下文切换，就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加 载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。

而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。 这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。

根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是**进程上下文切换**、**线程上下文切换**以及**中断上下文切换**。

进程上下文切换包括：1. 无法获取资源而导致的自愿上下文切换；2. 被系统强制调度导致的非自愿上下文切换。

CPU 上下文切换，是保证 Linux 系统正常工作的核心功能之一，一般情况下不需要我们特别关注。但过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，从而缩短进程真正运行的时间，导致系统的整体性能大幅下降。

## CPU性能指标
![[Pasted image 20220917110225.png|600]]
## CPU性能工具
![[Pasted image 20220917110258.png|700]]
![[Pasted image 20220917110322.png|700]]

**常用核心工具**
![[Pasted image 20220917110359.png|700]]

## CPU 优化
#### 1. 应用程序优化
**编译器优化**：很多编译器都会提供优化选项，适当开启它们，在编译阶段你就可以获得编 译器的帮助，来提升性能。比如， gcc 就提供了优化选项 -O2，开启后会自动对应用程 序的代码进行优化。

**算法优化**：使用复杂度更低的算法，可以显著加快处理速度。比如，在数据比较大的情况 下，可以用 O(nlogn) 的排序算法（如快排、归并排序等），代替 O(n^2) 的排序算法 （如冒泡、插入排序等）。

**异步处理**：使用异步处理，可以避免程序因为等待某个资源而一直阻塞，从而提升程序的 并发处理能力。比如，把轮询替换为事件通知，就可以避免轮询耗费 CPU 的问题。

**多线程代替多进程**：前面讲过，相对于进程的上下文切换，线程的上下文切换并不切换进 程地址空间，因此可以降低上下文切换的成本。

**善用缓存**：经常访问的数据或者计算过程中的步骤，可以放到内存中缓存起来，这样在下 次用时就能直接从内存中获取，加快程序的处理速度。

#### 2. 系统优化
**CPU 绑定**：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨 CPU 调度带来的上下文切换问题。

**CPU 独占**：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配 进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些 CPU。

**优先级调整**：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。优先级 的数值含义前面我们提到过，忘了的话及时复习一下。在这里，适当降低非核心应用的优 先级，增高核心应用的优先级，可以确保核心应用得到优先处理。

**为进程设置资源限制**：使用 Linux cgroups 来设置进程的 CPU 使用上限，可以防止由于 某个应用自身的问题，而耗尽系统资源。

**NUMA（Non-Uniform Memory Access）优化**：支持 NUMA 的处理器会被划分为 多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可 能只访问本地内存。

**中断负载均衡**：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的 CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均衡到多个 CPU 上。

