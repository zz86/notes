
## 神经网络&深度学习
神经网络：一种以（人工）神经元为基本单元的模型

深度学习：一类机器学习问题，主要解决贡献度分配问题。
![[Pasted image 20220922174952.png|200]]
深度学习模型训练的核心：**网络结构、损失函数、优化方法**

## 浅层学习
当我们用机器学习来解决一些模式识别任务时，一般的流程包含以下几个步骤：
![[Pasted image 20220922175146.png|700]]
**浅层学习**（Shallow Learning）：不涉及特征学习，其特征主要靠人工经验或特征转换方法来抽取。

## 深度学习
通过构建具有一定“深度”的模型，可以让模型来自动学习好的特征表示（从底层特征，到中层特征，再到高层特征），从而最终提升预测或识别的准确性。
![[Pasted image 20220922175344.png|700]]
#### 深度学习的数学描述
![[Pasted image 20220922175441.png|700]]
**神经网络天然不是深度学习，但深度学习天然是神经网络。**

## 人工神经元
![[Pasted image 20220922175623.png|700]]


## 神经网络结构
![[Pasted image 20220922200023.png|700]]
圆形节点表示一个神经元，方形节点表示一组神经元。

## 前馈神经网络  （全连接神经网络、多层感知器）
各神经元分别属于不同的层，层内无连接。

相邻两层之间的神经元全部两两连接。

整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。
![[Pasted image 20220922200323.png|500]]

前馈网络，也称为前馈神经网络。顾名思义，是一种“往前走”的神经网络。它是最简单 的神经网络，其典型特征是一个单向的多层结构。简化的结构如下图：
![[Pasted image 20220920141848.png|600]]
这个图中，你会看到最左侧的绿色 的一个个神经元，它们相当于第 0 层，一般适用于接收输入数据的层，所以我们把它们叫 做输入层。

比如我们要训练一个 y=f(x) 函数的神经网络，x 作为一个向量，就需要通过这个绿色的**输入层**进入模型。那么在这个网络中，输入层有 5 个神经元，这意味着它可以接收一个 5 维 长度的向量。

网络的中间有一层红色的神经元，它们相当于模型的“内 部”，一般来说对外不可见，或者使用者并不关心的非结果部分，我们称之为**隐藏层**。在 实际的网络模型中，隐藏层会有非常多的层数，它们是网络最为关键的内部核心，也是模 型能够学习知识的关键部分。

在图的右侧，蓝色的神经元是网络的最后一层。模型内部计算完成之后，就需要通过这一 层输出到外部，所以也叫做**输出层**。

需要说明的是，神经元之间的连线，表示神经元之间连接的权重，通过权重就会知道网络中每个节点的重要程度。

在前馈网络 中，数据从输入层进入到隐藏层的第一层，然后传播到第二层，第三层……一直到最后通过输出层输出。数据的传播是单向的，无法后退，只能前行。

前馈神经网络的训练过程可以分为以下三步

1. 前向计算每一层的状态和激活值，直到最后一层

2. 反向计算每一层的参数的偏导数

3. 更新参数

全连接神经网络的缺点：
![[Pasted image 20220922213503.png|700]]

## 卷积神经网络（Convolutional Neural Networks，CNN）
![[Pasted image 20220922221342.png|400]]
- 一种前馈神经网络
- 受生物学上感受野（Receptive Field）的机制而提出的
	- 在视觉神经系统中，一个神经元的感受野是指视网膜上的特定区域，只有这个区域内的刺激才能够激活该神经元。
	- 感受野主要是指听觉系统、本体感觉系统和视觉系统中神经元的一些性质。
- 全连接前馈神经网络中每一层都是全连接层，用卷积层代替全连接层就是卷积神经网络。

卷积神经网络有三个结构上的特性：
- 局部连接
- 权重共享
- 空间或时间上的次采样

卷积操作的目标：**提取特征**。

卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少。汇聚层可以显著减少神经元个数。

卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成。趋向于小卷积、大深度，趋向于全卷积。

典型结构：
![[Pasted image 20220922212840.png|800]]
一个卷积块为连续M 个卷积层和b个汇聚层（M通常设置为2 ∼ 5，b为0或1）。一个卷积网络中可以堆叠N 个连续的卷积块，然后在接着K 个全连接层（N 的取值区间比较大，比如1 ∼ 100或者更大；K一般为0 ∼ 2）。

#### 标准卷积
输入 m 个尺寸为 h, w 的特征图，通过卷积计算获得 n 个通道尺寸为 与 h′ w′ 的特征图的计算过程。
![[Pasted image 20220920110534.png|900]]
![[Pasted image 20220920124814.png]]
一个卷积核中的 m 个卷积分 别与输入特征图的 m 个通道数据进行卷积计算，生成一个中间结果，然后 m 个中间结果按位求和，最终就能获得 n 个输出特征图中的一个特征图。
![[Pasted image 20220920110605.png|900]]


#### 深度可分离卷积
简单来说，深度可 分离卷积就是我们刚才所说的在效果近似相同的情况下，需要的计算量更少。

深度可分离卷积（Depthwise Separable Convolution）由 **Depthwise（DW）和 Pointwise（PW）这两部分卷积组合而成的。

#### Depthwise（DW）卷积
DW 卷积就是有 m 个卷积核的卷积，每个卷积核中的通道数为 1，这 m 个卷积核分别与输入特征图对应的通道数据做卷积运算，所以 DW 卷积的输出是 有 m 个通道的特征图。通常来说，DW 卷积核的大小是 3x3 的。
![[Pasted image 20220920110701.png|900]]
#### Pointwise（PW）卷积
通常来说，深度可分离卷积的目标是轻量化标准卷积计算的，所以它是可以来替换标准卷 积的，这也意味着原卷积计算的输出尺寸是什么样，替换后的输出尺寸也要保持一致。

所以，在深度可分离卷积中，我们最终要获得一个具有 n 个通道的输出特征图，而刚才介 绍的 DW 卷积显然没有达到，并且 DW 卷积也忽略了输入特征图通道之间的信息。

所以，在 DW 之后我们还要加一个 PW 卷积。PW 卷积也叫做逐点卷积。PW 卷积的主要 作用就是将 DW 输出的 m 个特征图结合在一起考虑，再输出一个具有 n 个通道的特征 图。

在卷积神经网络中，我们经常可以看到使用 1x1 的卷积，1x1 的卷积主要作用就是升维与 降维。所以，在 DW 的输出之后的 PW 卷积，就是 n 个卷积核的 1x1 的卷积，每个卷积 核中有 m 个通道的卷积数据。
![[Pasted image 20220920123852.png|900]]

#### 计算量
![[Pasted image 20220920124125.png|900]]
![[Pasted image 20220920124059.png|600]]
#### 空洞卷积

如何增加输出单元的感受野?
- 增加卷积核的大小
- 增加层数来实现
- 在卷积之前进行汇聚操作
- 空洞卷积

空洞卷积经常用于图像分割任务当中。图像分割任务的目的是要做到 pixel-wise 的输出， 也就是说，对于图片中的每一个像素点，模型都要进行预测。

对于一个图像分割模型，通常会采用多层卷积来提取特征的，随着层数的不断加深，感受野也越来越大。

但是对于图像分割模型有个问题，经过多层的卷积与 pooling 操作之后，特征图会变小。 为了做到每个像素点都有预测输出，我们需要对较小的特征图进行上采样或反卷积，将特 征图扩大到一定尺度，然后再进行预测。

要知道，从一个较小的特征图恢复到一个较大的特征图，这显然会带来一定的信息损失， 特别是较小的物体，这种损失是很难恢复的。那问题来了，能不能既保证有比较大的感受野，同时又不用缩小特征图呢？

估计你已经猜到了，空洞卷积就是解决这个问题的杀手锏，它最大的优点就是不需要缩小特征图，也可以获得更大的感受野。
![[Pasted image 20220920124834.png|300]]

#### 转置卷积/微步卷积
低维特征映射到高维特征
![[Pasted image 20220922214529.png|300]]

## 循环神经网络（ Recurrent Neural Network ，RNN ）
循环神经网络通过使用带自反馈的神经元，能够处理任意长度的时序数据。
![[Pasted image 20220922221535.png|600]]
循环神经网络比前馈神经网络更加符合生物神经网络的结构。

循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上

循环神经网络的作用：
1. 输入-输出映射：机器学习模型
2. 存储器：联想记忆模型

将循环神经网络应用到机器学习：
1. 序列到类别：情感分类

2. 同步的序列到序列模式：中文分词、信息抽取、语音识别

3. 异步的序列到序列模式：机器翻译

##### 循环神经网络总结
优点：引入记忆、图灵完备

缺点：长程依赖问题、记忆容量问题、并行能力

## 损失函数和代价函数
损失函数是评价拟合函数表现效果“好坏”的度量指标，是单个样本点的误差。

代价函数是在训练样本集合上，所有样本的拟合误差的平均值。

严格来说，损失函数的种类是无穷多的。这是因为损失函数是用来度量模型拟合效 果和真实值之间的差距，而度量方式要根据问题的特点或者需要优化的方面具体定制，所 以损失函数的种类是无穷无尽的。

常见的损失函数有：0-1 损失函数、平方损失函数、均方差损失函数和平均绝对误差损失函数、交叉熵损失函数、softmax 损失函数等。

## 反向传播
模型就是通过不断地减小损失函数值的方式来进行学习的。让损失函数最小化，通常就要采用**梯度下降**的方式，即：每一次给模型的权重进行更新的时候，都要按照梯度的反方向进行。

为什么呢？因为梯度向量的方向即为函数值增长最快的方向，反方向则是减小最快的方向。

上面这个自然段的内容非常非常核心，为了确保你学会，我们换个方式再说一次：模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习。

反向传播算法（Backpropagation）是目前训练神经网络最常用且最有效的算法。模型就 是通过反向传播的方式来不断更新自身的参数，从而实现了“学习”知识的过程。

反向传播的主要原理是：

- 前向传播：数据从输入层经过隐藏层最后输出，其过程和之前讲过的前馈网络基本一 致。

- 计算误差并传播：计算模型输出结果和真实结果之间的误差，并将这种误差通过某种方式反向传播，即从输出层向隐藏层传递并最后到达输入层。

- 迭代：在反向传播的过程中，根据误差不断地调整模型的参数值，并不断地迭代前面两个步骤，直到达到模型结束训练的条件。

其中最重要的环节有两个：一是通过某种方式反向传播；二是根据误差不断地调整模型的参数值。

这两个环节，我们统称为**优化方法**，一般而言，多采用**梯度下降**的方法。



## 三个深度学习神经网络性能优化的方法

第一个方法是针对图形数据做增广，也就是把有限的数据个数变多。其实不只是 CNN 网络，对于其他神经网络模型，乃至神经网络模型之外的模型来说，数据的数量都是多多益善。数据的量越大，模型出现过拟合的风险就越小。

第二个方法是在网络模型中增加一些 Dropout 层，这种类型的神经网络层，通过阻止各个 神经元之间的共同作用，阻止它们形成固定的特征提取模式来提高神经网络的泛化能力。

第三个方法是尝试使用不同类型的神经网络优化器，来克服网络训练时落入局部最低点的问题。其中，最常用的神经网络优化器是 Adam，但具体到每一个数据集来说，可能还有 更适合自己的优化器，因此，我建议你在实际项目中最好还是多试试不同的优化器。就目前而言，RMSprop 和 Adam 都是常用的优化器，而 Adam 更是多种优化思路的集大成者，一般情况下是优化器的首选项。
![[Pasted image 20220907221507.png|700]]

## 图像分类
![[Pasted image 20220921200130.png|700]]
中间绿色的节点叫做神经元，是感知机的最基本组成单元。上图中的感知机只有中间一层（绿色的神经元），如果有多层神经元的话，我们就称之为多层感知机。

那什么是神经元呢？神经元是关于输入的一个线性变换，每一个输入 x 都会有一个对应的 权值，上图中的 y 的计算方式为：
![[Pasted image 20220921200511.png|700]]
经过 Softmax 之后，原始的输出 y 是不是转换成一组概率，并且概率的和为 1

当然，Softmax 也不是每一个问题都会使用。我们根据问题的不同可以采用不同的函数， 例如，有的时候也会使用 sigmoid 激活函数，sigmoid 激活函数是将 1 个数值转换为 0 到 1 之间的概率。

**全连接层**，Full Connection Layer，简称 fc 层。一般都是放在网络的最后端，用来获得最终的输出，也就是各个类别的概率。

因为全连接层中的神经元的个数是固定的，所以说在有全连接层的网络中，输入图片是必 须固定尺寸的。而现实里我们线上收集到的图片会有不同的尺寸，所以需要先把图片尺寸统一起来，PyTorch 才能进一步处理。

PyTorch 中全连接层用 nn.Linear 来实现。我们分别看看里面的重要参数有哪些：

in_features：输入特征的个数，在本例中为 128x128；

out_features：输出的特征数，在本例中为 2；

bias：是否需要偏移项，默认为 True。

全连接层的输入，也不是原始图片数据，而是经过多层卷积提取的特征。

前面我们曾说过，有的网络是可以接收任意尺度的输入的。在上文中的设计中，全连接层 的输入 x1 到 xn 是固定的，数目等于最后一层特征图所有元素的数目。

![[Pasted image 20220921200008.png|600]]
输入图片经过卷积层提取特征后，最终会生成若干特征图，然后在这些特征图之后会接一个全连接层（上图中红色的圆圈），全连接层中的节点数就对应着要将图片分为几类。我们将全连接层的输出送入到 softmax 中，就可以获得每个类别的概率，然后通过概率就可以判断输入图片属于哪一个类别了。

## 图像分割
图像分类是将一张图片自动分成某一类别，而图像分割是需要将图片中的每一个像素进行分类。

图像分割可以分为**语义分割与实例分割**，两者的区别是语义分割中只需要把每个像素点进 行分类就可以了，不需要区分是否来自同一个实例，而实例分割不仅仅需要对像素点进行 分类，还需要判断来自哪个实例。

如下图所示，左侧为语义分割，右侧为实例分割。
![[Pasted image 20220921195936.png|600]]
在图像分割中，同样是利用卷积层来提取特征，最终生成若干特征图。只不过最后生成的特征图

的数目对应着要分割成的类别数。举一个例子，假设我们想要将输入的小猫分割出来，也就是说，这个图像分割模型有两个类别，分别是小猫与背景，如下图所示。
![[Pasted image 20220921200639.png|800]]
最终的两个特征图中，通道 1 代表的小猫的信息，通道 2 对应着背景的信息。

这里我给你再举一个例子，来说明一下如何判断每个像素的类别。假设，通道 1 中（0,0） 这个位置的输出是 2，通道 2 中（0,0）这个位置的输出是 30。

经过 softmax 转为概率后，通道 1（0, 0）这个位置的概率为 0，而对应通道 2 中 (0,0) 这 个位置的概率为 1，我们通过概率可以判断出，在（0，0）这个位置是背景，而不是小猫。

##### 网络结构

在分割网络中最终输出的特征图的大小要么是与输入的原图相同，要么就是接近输入。

这么做的原因是，我们要对原图中的每个像素进行判断。当输出特征图与原图尺寸相同 时，可以直接进行分割判断。当输出特征图与原图尺寸不相同时，需要将输出的特征图 resize 到原图大小。

如果是从一个比较小的特征图 resize 到一个比较大的尺寸的时候，必定会丢失掉一部分信 息的。所以，输出特征图的大小不能太小。

这也是图像分割网络与图像分类网络的第二个不同点，在图像分类中，经过多层的特征提 取，最后生成的特征图都是很小的。而在图像分割中，最后生成的特征图通常来说是接近 原图的。

前文也说过，图像分割网络也是通过卷积进行提取特征的，按照之前的理论特征提取后， 特征图尺寸是减小的。如果说把特征提取看做 Encoder 的话，那在图像分割中还有一步是 Decoder。

Decoder 的作用就是对特征图尺寸进行还原的过程，将尺寸还原到一个比较大的尺寸。这 个还原的操作对应的就是上采样。而在上采样中我们通常使用的是转置卷积。

##### 转置卷积
转置卷积的计算过程如下：

1. 对输入特征图进行补零操作。

2. 将转置卷积的卷积核上下、左右变换作为新的卷积核。

3. 利用新的卷积核在 1 的基础上进行步长为 1，padding 为 0 的卷积操作。

##### 损失函数
在图像分割中依然可以使用在图像分类中经常使用的交叉熵损失。在图像分类中，一张图 片有一个预测结果，预测结果与真实值就可以计算出一个 Loss。而在图像分割中，真实的 标签是一张二维特征图，这张特征图记录着每个像素的真实分类结果。在分割中，含有像 素类别的特征图，我们一般称为 Mask。

我们结合一张小猫图片的例子解释一下。对于下图中的小猫进行标记，标记后会生成它的 GT，这个 GT 就是一个 Mask。

GT 是 Ground Truth 的缩写，在图像分割中我们经常使用这个词。在图像分类中与之对应 的就是数据的真实标签，在图像分割中则 GT 是每个像素的真实分类，如下面的例子所示。

那在我们模型预测的 Mask 中，每个位置都会有一个预测结果，这个预测结果与 GT 中的 Mask 做比较，然后会生成一个 Loss。

当然，在图像分割中不光有交叉熵损失可以用，还可以用更加有针对性的 Dice Loss。

##### 语义分割与图像分类主要有两个不同点：

1. 在分类端有所不同，在图像分类中，经过卷积的特征提取后，最后会以若干个神经元的 形式作为输出，每个神经元代表着对一个类别的判断情况。而语义分割，则是会输出若干 的特征图，每个特征图代表着对应类别判断。

2. 在图像分类的网络中，特征图是不断减小的。但是在语义分割的网络中，特征图还会有 decoder 这一步，它是将特征图进行放大的过程。实现 decoder 的方式称为上采样，在上 采样中我们最常使用的就是转置卷积。

对于转置卷积，除了要知道它是怎么计算的之外，最重要的是要记住它**不是卷积的逆运算，只是能将特征图的大小进行放大的一种卷积运算**。

