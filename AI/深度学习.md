
深度学习模型训练的核心：**网络结构、损失函数、优化方法**

## 标准卷积
输入 m 个尺寸为 h, w 的特征图，通过卷积计算获得 n 个通道尺寸为 与 h′ w′ 的特征图的计算过程。
![[Pasted image 20220920110534.png|900]]
![[Pasted image 20220920124814.png]]
一个卷积核中的 m 个卷积分 别与输入特征图的 m 个通道数据进行卷积计算，生成一个中间结果，然后 m 个中间结果 按位求和，最终就能获得 n 个输出特征图中的一个特征图。
![[Pasted image 20220920110605.png|900]]


## 深度可分离卷积
简单来说，深度可 分离卷积就是我们刚才所说的在效果近似相同的情况下，需要的计算量更少。

深度可分离卷积（Depthwise Separable Convolution）由 **Depthwise（DW）和 Pointwise（PW）这两部分卷积组合而成的。

#### Depthwise（DW）卷积
DW 卷积就是有 m 个卷积核的卷积，每个卷积核中的通道数为 1，这 m 个卷积核分别与输入特征图对应的通道数据做卷积运算，所以 DW 卷积的输出是 有 m 个通道的特征图。通常来说，DW 卷积核的大小是 3x3 的。
![[Pasted image 20220920110701.png|900]]
#### Pointwise（PW）卷积
通常来说，深度可分离卷积的目标是轻量化标准卷积计算的，所以它是可以来替换标准卷 积的，这也意味着原卷积计算的输出尺寸是什么样，替换后的输出尺寸也要保持一致。

所以，在深度可分离卷积中，我们最终要获得一个具有 n 个通道的输出特征图，而刚才介 绍的 DW 卷积显然没有达到，并且 DW 卷积也忽略了输入特征图通道之间的信息。

所以，在 DW 之后我们还要加一个 PW 卷积。PW 卷积也叫做逐点卷积。PW 卷积的主要 作用就是将 DW 输出的 m 个特征图结合在一起考虑，再输出一个具有 n 个通道的特征 图。

在卷积神经网络中，我们经常可以看到使用 1x1 的卷积，1x1 的卷积主要作用就是升维与 降维。所以，在 DW 的输出之后的 PW 卷积，就是 n 个卷积核的 1x1 的卷积，每个卷积 核中有 m 个通道的卷积数据。
![[Pasted image 20220920123852.png|900]]

#### 计算量
![[Pasted image 20220920124125.png|900]]
![[Pasted image 20220920124059.png|600]]
## 空洞卷积
空洞卷积经常用于图像分割任务当中。图像分割任务的目的是要做到 pixel-wise 的输出， 也就是说，对于图片中的每一个像素点，模型都要进行预测。

对于一个图像分割模型，通常会采用多层卷积来提取特征的，随着层数的不断加深，感受野也越来越大。

但是对于图像分割模型有个问题，经过多层的卷积与 pooling 操作之后，特征图会变小。 为了做到每个像素点都有预测输出，我们需要对较小的特征图进行上采样或反卷积，将特 征图扩大到一定尺度，然后再进行预测。

要知道，从一个较小的特征图恢复到一个较大的特征图，这显然会带来一定的信息损失， 特别是较小的物体，这种损失是很难恢复的。那问题来了，能不能既保证有比较大的感受野，同时又不用缩小特征图呢？

估计你已经猜到了，空洞卷积就是解决这个问题的杀手锏，它最大的优点就是不需要缩小特征图，也可以获得更大的感受野。
![[Pasted image 20220920124834.png]]
## 损失函数和代价函数
损失函数是评价拟合函数表现效果“好坏”的度量指标，是单个样本点的误差。

代价函数是在训练样本集合上，所有样本的拟合误差的平均值。

严格来说，损失函数的种类是无穷多的。这是因为损失函数是用来度量模型拟合效 果和真实值之间的差距，而度量方式要根据问题的特点或者需要优化的方面具体定制，所 以损失函数的种类是无穷无尽的。

常见的损失函数有：0-1 损失函数、平方损失函数、均方差损失函数和平均绝对误差损失函数、交叉熵损失函数、softmax 损失函数等。

## 前馈网络  （前向网络、前向传播）
前馈网络，也称为前馈神经网络。顾名思义，是一种“往前走”的神经网络。它是最简单 的神经网络，其典型特征是一个单向的多层结构。简化的结构如下图：
![[Pasted image 20220920141848.png|600]]
这个图中，你会看到最左侧的绿色 的一个个神经元，它们相当于第 0 层，一般适用于接收输入数据的层，所以我们把它们叫 做输入层。

比如我们要训练一个 y=f(x) 函数的神经网络，x 作为一个向量，就需要通过这个绿色的输 入层进入模型。那么在这个网络中，输入层有 5 个神经元，这意味着它可以接收一个 5 维 长度的向量。

网络的中间有一层红色的神经元，它们相当于模型的“内 部”，一般来说对外不可见，或者使用者并不关心的非结果部分，我们称之为隐藏层。在 实际的网络模型中，隐藏层会有非常多的层数，它们是网络最为关键的内部核心，也是模 型能够学习知识的关键部分。

在图的右侧，蓝色的神经元是网络的最后一层。模型内部计算完成之后，就需要通过这一 层输出到外部，所以也叫做输出层。

需要说明的是，神经元之间的连线，表示神经元之间连接的权重，通过权重就会知道网络 中每个节点的重要程度。

在前馈网络 中，数据从输入层进入到隐藏层的第一层，然后传播到第二层，第三层……一直到最后通过输出层输出。数据的传播是单向的，无法后退，只能前行。

## 反向传播
模型就是通过不断地减小损失函数值的方式来进行学习的。让损失函数最小化，通常就要采用梯度下降的方式，即：每一次给模型的权重进行更新的时候，都要按照梯度的反方向进行。

为什么呢？因为梯度向量的方向即为函数值增长最快的方向，反方向则是减小最快的方向。

上面这个自然段的内容非常非常核心，为了确保你学会，我们换个方式再说一次：模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习。

反向传播算法（Backpropagation）是目前训练神经网络最常用且最有效的算法。模型就 是通过反向传播的方式来不断更新自身的参数，从而实现了“学习”知识的过程。

反向传播的主要原理是：

- 前向传播：数据从输入层经过隐藏层最后输出，其过程和之前讲过的前馈网络基本一 致。

- 计算误差并传播：计算模型输出结果和真实结果之间的误差，并将这种误差通过某种方式反向传播，即从输出层向隐藏层传递并最后到达输入层。

- 迭代：在反向传播的过程中，根据误差不断地调整模型的参数值，并不断地迭代前面两个步骤，直到达到模型结束训练的条件。

其中最重要的环节有两个：一是通过某种方式反向传播；二是根据误差不断地调整模型的参数值。

这两个环节，我们统称为**优化方法**，一般而言，多采用梯度下降的方法。



## 三个深度学习神经网络性能优化的方法

第一个方法是针对图形数据做增广，也就是把有限的数据个数变多。其实不只是 CNN 网络，对于其他神经网络模型，乃至神经网络模型之外的模型来说，数据的数量都是多多益善。数据的量越大，模型出现过拟合的风险就越小。

第二个方法是在网络模型中增加一些 Dropout 层，这种类型的神经网络层，通过阻止各个 神经元之间的共同作用，阻止它们形成固定的特征提取模式来提高神经网络的泛化能力。

第三个方法是尝试使用不同类型的神经网络优化器，来克服网络训练时落入局部最低点的问题。其中，最常用的神经网络优化器是 Adam，但具体到每一个数据集来说，可能还有 更适合自己的优化器，因此，我建议你在实际项目中最好还是多试试不同的优化器。就目前而言，RMSprop 和 Adam 都是常用的优化器，而 Adam 更是多种优化思路的集大成者，一般情况下是优化器的首选项。
![[Pasted image 20220907221507.png|700]]

## 图像分类
![[Pasted image 20220921200130.png|700]]
中间绿色的节点叫做神经元，是感知机的最基本组成单元。上图中的感知机只有中间一层（绿色的神经元），如果有多层神经元的话，我们就称之为多层感知机。

那什么是神经元呢？神经元是关于输入的一个线性变换，每一个输入 x 都会有一个对应的 权值，上图中的 y 的计算方式为：
![[Pasted image 20220921200511.png|700]]
经过 Softmax 之后，原始的输出 y 是不是转换成一组概率，并且概率的和为 1

当然，Softmax 也不是每一个问题都会使用。我们根据问题的不同可以采用不同的函数， 例如，有的时候也会使用 sigmoid 激活函数，sigmoid 激活函数是将 1 个数值转换为 0 到 1 之间的概率。

**全连接层**，Full Connection Layer，简称 fc 层。一般都是放在网络的最后端，用来获得最终的输出，也就是各个类别的概率。

因为全连接层中的神经元的个数是固定的，所以说在有全连接层的网络中，输入图片是必 须固定尺寸的。而现实里我们线上收集到的图片会有不同的尺寸，所以需要先把图片尺寸统一起来，PyTorch 才能进一步处理。

PyTorch 中全连接层用 nn.Linear 来实现。我们分别看看里面的重要参数有哪些：

in_features：输入特征的个数，在本例中为 128x128；

out_features：输出的特征数，在本例中为 2；

bias：是否需要偏移项，默认为 True。

全连接层的输入，也不是原始图片数据，而是经过多层卷积提取的特征。

前面我们曾说过，有的网络是可以接收任意尺度的输入的。在上文中的设计中，全连接层 的输入 x1 到 xn 是固定的，数目等于最后一层特征图所有元素的数目。

![[Pasted image 20220921200008.png|600]]
输入图片经过卷积层提取特征后，最终会生成若干特征图，然后在这些特征图之后会接一个全连接层（上图中红色的圆圈），全连接层中的节点数就对应着要将图片分为几类。我们将全连接层的输出送入到 softmax 中，就可以获得每个类别的概率，然后通过概率就可以判断输入图片属于哪一个类别了。

## 图像分割
图像分类是将一张图片自动分成某一类别，而图像分割是需要将图片中的每一个像素进行分类。

图像分割可以分为**语义分割与实例分割**，两者的区别是语义分割中只需要把每个像素点进 行分类就可以了，不需要区分是否来自同一个实例，而实例分割不仅仅需要对像素点进行 分类，还需要判断来自哪个实例。

如下图所示，左侧为语义分割，右侧为实例分割。
![[Pasted image 20220921195936.png|600]]
在图像分割中，同样是利用卷积层来提取特征，最终生成若干特征图。只不过最后生成的特征图

的数目对应着要分割成的类别数。举一个例子，假设我们想要将输入的小猫分割出来，也就是说，这个图像分割模型有两个类别，分别是小猫与背景，如下图所示。
![[Pasted image 20220921200639.png|800]]
最终的两个特征图中，通道 1 代表的小猫的信息，通道 2 对应着背景的信息。

这里我给你再举一个例子，来说明一下如何判断每个像素的类别。假设，通道 1 中（0,0） 这个位置的输出是 2，通道 2 中（0,0）这个位置的输出是 30。

经过 softmax 转为概率后，通道 1（0, 0）这个位置的概率为 0，而对应通道 2 中 (0,0) 这 个位置的概率为 1，我们通过概率可以判断出，在（0，0）这个位置是背景，而不是小猫。

##### 网络结构

在分割网络中最终输出的特征图的大小要么是与输入的原图相同，要么就是接近输入。

这么做的原因是，我们要对原图中的每个像素进行判断。当输出特征图与原图尺寸相同 时，可以直接进行分割判断。当输出特征图与原图尺寸不相同时，需要将输出的特征图 resize 到原图大小。

如果是从一个比较小的特征图 resize 到一个比较大的尺寸的时候，必定会丢失掉一部分信 息的。所以，输出特征图的大小不能太小。

这也是图像分割网络与图像分类网络的第二个不同点，在图像分类中，经过多层的特征提 取，最后生成的特征图都是很小的。而在图像分割中，最后生成的特征图通常来说是接近 原图的。

前文也说过，图像分割网络也是通过卷积进行提取特征的，按照之前的理论特征提取后， 特征图尺寸是减小的。如果说把特征提取看做 Encoder 的话，那在图像分割中还有一步是 Decoder。

Decoder 的作用就是对特征图尺寸进行还原的过程，将尺寸还原到一个比较大的尺寸。这 个还原的操作对应的就是上采样。而在上采样中我们通常使用的是转置卷积。

##### 转置卷积
转置卷积的计算过程如下：

1. 对输入特征图进行补零操作。

2. 将转置卷积的卷积核上下、左右变换作为新的卷积核。

3. 利用新的卷积核在 1 的基础上进行步长为 1，padding 为 0 的卷积操作。

##### 损失函数
在图像分割中依然可以使用在图像分类中经常使用的交叉熵损失。在图像分类中，一张图 片有一个预测结果，预测结果与真实值就可以计算出一个 Loss。而在图像分割中，真实的 标签是一张二维特征图，这张特征图记录着每个像素的真实分类结果。在分割中，含有像 素类别的特征图，我们一般称为 Mask。

我们结合一张小猫图片的例子解释一下。对于下图中的小猫进行标记，标记后会生成它的 GT，这个 GT 就是一个 Mask。

GT 是 Ground Truth 的缩写，在图像分割中我们经常使用这个词。在图像分类中与之对应 的就是数据的真实标签，在图像分割中则 GT 是每个像素的真实分类，如下面的例子所示。

那在我们模型预测的 Mask 中，每个位置都会有一个预测结果，这个预测结果与 GT 中的 Mask 做比较，然后会生成一个 Loss。

当然，在图像分割中不光有交叉熵损失可以用，还可以用更加有针对性的 Dice Loss。

##### 语义分割与图像分类主要有两个不同点：

1. 在分类端有所不同，在图像分类中，经过卷积的特征提取后，最后会以若干个神经元的 形式作为输出，每个神经元代表着对一个类别的判断情况。而语义分割，则是会输出若干 的特征图，每个特征图代表着对应类别判断。

2. 在图像分类的网络中，特征图是不断减小的。但是在语义分割的网络中，特征图还会有 decoder 这一步，它是将特征图进行放大的过程。实现 decoder 的方式称为上采样，在上 采样中我们最常使用的就是转置卷积。

对于转置卷积，除了要知道它是怎么计算的之外，最重要的是要记住它**不是卷积的逆运算，只是能将特征图的大小进行放大的一种卷积运算**。

